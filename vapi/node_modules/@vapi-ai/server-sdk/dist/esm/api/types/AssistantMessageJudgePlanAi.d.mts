import type * as Vapi from "../index.mjs";
export interface AssistantMessageJudgePlanAi {
    /**
     * This is the model to use for the LLM-as-a-judge.
     * If not provided, will default to the assistant's model.
     *
     * The instructions on how to evaluate the model output with this LLM-Judge must be passed as a system message in the messages array of the model.
     *
     * The Mock conversation can be passed to the LLM-Judge to evaluate using the prompt {{messages}} and will be evaluated as a LiquidJS Variable. To access and judge only the last message, use {{messages[-1]}}
     *
     * The LLM-Judge must respond with "pass" or "fail" and only those two responses are allowed.
     */
    model: Vapi.AssistantMessageJudgePlanAiModel;
    /**
     * This is the type of the judge plan.
     * Use 'ai' to evaluate the assistant message content using LLM-as-a-judge.
     * @default 'ai'
     */
    type: Vapi.AssistantMessageJudgePlanAiType;
}
