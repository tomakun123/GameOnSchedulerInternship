import type * as Vapi from "../index.mjs";
export interface EvalAnthropicModel {
    /** This is the provider of the model (`anthropic`). */
    provider: Vapi.EvalAnthropicModelProvider;
    /** This is the specific model that will be used. */
    model: Vapi.EvalAnthropicModelModel;
    /**
     * This is the optional configuration for Anthropic's thinking feature.
     *
     * - Only applicable for `claude-3-7-sonnet-20250219` model.
     * - If provided, `maxTokens` must be greater than `thinking.budgetTokens`.
     */
    thinking?: Vapi.AnthropicThinkingConfig;
    /** This is the temperature of the model. For LLM-as-a-judge, it's recommended to set it between 0 - 0.3 to avoid hallucinations and ensure the model judges the output correctly based on the instructions. */
    temperature?: number;
    /**
     * This is the max tokens of the model.
     * If your Judge instructions return `true` or `false` takes only 1 token (as per the OpenAI Tokenizer), and therefore is recommended to set it to a low number to force the model to return a short response.
     */
    maxTokens?: number;
    /**
     * These are the messages which will instruct the AI Judge on how to evaluate the assistant message.
     * The LLM-Judge must respond with "pass" or "fail" to indicate if the assistant message passes the eval.
     *
     * To access the messages in the mock conversation, use the LiquidJS variable `{{messages}}`.
     * The assistant message to be evaluated will be passed as the last message in the `messages` array and can be accessed using `{{messages[-1]}}`.
     *
     * It is recommended to use the system message to instruct the LLM how to evaluate the assistant message, and then use the first user message to pass the assistant message to be evaluated.
     */
    messages: Record<string, unknown>[];
}
