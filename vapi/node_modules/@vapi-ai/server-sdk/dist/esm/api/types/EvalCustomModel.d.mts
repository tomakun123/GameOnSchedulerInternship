import type * as Vapi from "../index.mjs";
export interface EvalCustomModel {
    /** This is the provider of the model (`custom-llm`). */
    provider: Vapi.EvalCustomModelProvider;
    /** These is the URL we'll use for the OpenAI client's `baseURL`. Ex. https://openrouter.ai/api/v1 */
    url: string;
    /** These are the headers we'll use for the OpenAI client's `headers`. */
    headers?: Record<string, unknown>;
    /** This sets the timeout for the connection to the custom provider without needing to stream any tokens back. Default is 20 seconds. */
    timeoutSeconds?: number;
    /** This is the name of the model. Ex. gpt-4o */
    model: string;
    /** This is the temperature of the model. For LLM-as-a-judge, it's recommended to set it between 0 - 0.3 to avoid hallucinations and ensure the model judges the output correctly based on the instructions. */
    temperature?: number;
    /**
     * This is the max tokens of the model.
     * If your Judge instructions return `true` or `false` takes only 1 token (as per the OpenAI Tokenizer), and therefore is recommended to set it to a low number to force the model to return a short response.
     */
    maxTokens?: number;
    /**
     * These are the messages which will instruct the AI Judge on how to evaluate the assistant message.
     * The LLM-Judge must respond with "pass" or "fail" to indicate if the assistant message passes the eval.
     *
     * To access the messages in the mock conversation, use the LiquidJS variable `{{messages}}`.
     * The assistant message to be evaluated will be passed as the last message in the `messages` array and can be accessed using `{{messages[-1]}}`.
     *
     * It is recommended to use the system message to instruct the LLM how to evaluate the assistant message, and then use the first user message to pass the assistant message to be evaluated.
     */
    messages: Record<string, unknown>[];
}
