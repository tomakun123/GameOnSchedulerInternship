import type * as Vapi from "../index.js";
export interface WorkflowCustomModel {
    /** This is the provider of the model (`custom-llm`). */
    provider: Vapi.WorkflowCustomModelProvider;
    /**
     * This determines whether metadata is sent in requests to the custom provider.
     *
     * - `off` will not send any metadata. payload will look like `{ messages }`
     * - `variable` will send `assistant.metadata` as a variable on the payload. payload will look like `{ messages, metadata }`
     * - `destructured` will send `assistant.metadata` fields directly on the payload. payload will look like `{ messages, ...metadata }`
     *
     * Further, `variable` and `destructured` will send `call`, `phoneNumber`, and `customer` objects in the payload.
     *
     * Default is `variable`.
     */
    metadataSendMode?: Vapi.WorkflowCustomModelMetadataSendMode;
    /** These is the URL we'll use for the OpenAI client's `baseURL`. Ex. https://openrouter.ai/api/v1 */
    url: string;
    /** These are the headers we'll use for the OpenAI client's `headers`. */
    headers?: Record<string, unknown>;
    /** This sets the timeout for the connection to the custom provider without needing to stream any tokens back. Default is 20 seconds. */
    timeoutSeconds?: number;
    /** This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
    model: string;
    /** This is the temperature of the model. */
    temperature?: number;
    /** This is the max tokens of the model. */
    maxTokens?: number;
}
